{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "912dd602-c212-4121-852e-19fd714521e0",
     "showTitle": true,
     "title": "Spark Version"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").getOrCreate()\n",
    "print(spark.sparkContext.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84c065d9-af04-4a84-820a-7dd37e3412db",
     "showTitle": true,
     "title": "Create Dataframe dF1"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>name</th><th>doj</th><th>employee_dept_id</th><th>gender</th><th>salary</th></tr></thead><tbody><tr><td>100</td><td>Stephen</td><td>1999</td><td>100</td><td>M</td><td>2000</td></tr><tr><td>200</td><td>Philipp</td><td>2002</td><td>200</td><td>M</td><td>8000</td></tr><tr><td>300</td><td>John</td><td>2010</td><td>100</td><td></td><td>6000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         100,
         "Stephen",
         "1999",
         "100",
         "M",
         2000
        ],
        [
         200,
         "Philipp",
         "2002",
         "200",
         "M",
         8000
        ],
        [
         300,
         "John",
         "2010",
         "100",
         "",
         6000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doj",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_dept_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "employee_data = [(100,\"Stephen\",\"1999\",\"100\",\"M\",2000),\n",
    "                 (200,\"Philipp\",\"2002\",\"200\",\"M\",8000),\n",
    "                 (300,\"John\",\"2010\",\"100\",\"\",6000)\n",
    "                 ]\n",
    "\n",
    "employee_schema = [\"employee_id\",\"name\",\"doj\",\"employee_dept_id\",\"gender\",\"salary\"]\n",
    "\n",
    "dF1 = spark.createDataFrame(data = employee_data, schema = employee_schema)\n",
    "\n",
    "display(dF1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb21cae3-adfc-490f-80c0-48d174d3e28f",
     "showTitle": true,
     "title": "Create DataFrame dF2"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>name</th><th>doj</th><th>employee_dept_id</th><th>gender</th><th>salary</th></tr></thead><tbody><tr><td>300</td><td>John</td><td>2010</td><td>100</td><td></td><td>6000</td></tr><tr><td>400</td><td>Nancy</td><td>2008</td><td>400</td><td>F</td><td>1000</td></tr><tr><td>500</td><td>Rosy</td><td>2014</td><td>500</td><td>M</td><td>5000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         300,
         "John",
         "2010",
         "100",
         "",
         6000
        ],
        [
         400,
         "Nancy",
         "2008",
         "400",
         "F",
         1000
        ],
        [
         500,
         "Rosy",
         "2014",
         "500",
         "M",
         5000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doj",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_dept_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "employee_data = [(300,\"John\",\"2010\",\"100\",\"\",6000),\n",
    "                 (400,\"Nancy\",\"2008\",\"400\",\"F\",1000),\n",
    "                 (500,\"Rosy\",\"2014\",\"500\",\"M\",5000)\n",
    "                 ]\n",
    "\n",
    "employee_schema = [\"employee_id\",\"name\",\"doj\",\"employee_dept_id\",\"gender\",\"salary\"]\n",
    "\n",
    "dF2 = spark.createDataFrame(data = employee_data, schema = employee_schema)\n",
    "\n",
    "display(dF2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06e2a859-dfc3-41f6-a523-3928a7869faf",
     "showTitle": true,
     "title": "Union"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>name</th><th>doj</th><th>employee_dept_id</th><th>gender</th><th>salary</th></tr></thead><tbody><tr><td>100</td><td>Stephen</td><td>1999</td><td>100</td><td>M</td><td>2000</td></tr><tr><td>200</td><td>Philipp</td><td>2002</td><td>200</td><td>M</td><td>8000</td></tr><tr><td>300</td><td>John</td><td>2010</td><td>100</td><td></td><td>6000</td></tr><tr><td>300</td><td>John</td><td>2010</td><td>100</td><td></td><td>6000</td></tr><tr><td>400</td><td>Nancy</td><td>2008</td><td>400</td><td>F</td><td>1000</td></tr><tr><td>500</td><td>Rosy</td><td>2014</td><td>500</td><td>M</td><td>5000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         100,
         "Stephen",
         "1999",
         "100",
         "M",
         2000
        ],
        [
         200,
         "Philipp",
         "2002",
         "200",
         "M",
         8000
        ],
        [
         300,
         "John",
         "2010",
         "100",
         "",
         6000
        ],
        [
         300,
         "John",
         "2010",
         "100",
         "",
         6000
        ],
        [
         400,
         "Nancy",
         "2008",
         "400",
         "F",
         1000
        ],
        [
         500,
         "Rosy",
         "2014",
         "500",
         "M",
         5000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doj",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_dept_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dF_union = dF1.union(dF2)\n",
    "display(dF_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d38126b-aefa-4401-b58b-be9eaaeb9227",
     "showTitle": true,
     "title": "Drop Duplicates"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>name</th><th>doj</th><th>employee_dept_id</th><th>gender</th><th>salary</th></tr></thead><tbody><tr><td>100</td><td>Stephen</td><td>1999</td><td>100</td><td>M</td><td>2000</td></tr><tr><td>200</td><td>Philipp</td><td>2002</td><td>200</td><td>M</td><td>8000</td></tr><tr><td>300</td><td>John</td><td>2010</td><td>100</td><td></td><td>6000</td></tr><tr><td>400</td><td>Nancy</td><td>2008</td><td>400</td><td>F</td><td>1000</td></tr><tr><td>500</td><td>Rosy</td><td>2014</td><td>500</td><td>M</td><td>5000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         100,
         "Stephen",
         "1999",
         "100",
         "M",
         2000
        ],
        [
         200,
         "Philipp",
         "2002",
         "200",
         "M",
         8000
        ],
        [
         300,
         "John",
         "2010",
         "100",
         "",
         6000
        ],
        [
         400,
         "Nancy",
         "2008",
         "400",
         "F",
         1000
        ],
        [
         500,
         "Rosy",
         "2014",
         "500",
         "M",
         5000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doj",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_dept_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dF_union.dropDuplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d8aa485-0a94-407b-8644-67a55122d8ef",
     "showTitle": true,
     "title": "UnionAll"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>name</th><th>doj</th><th>employee_dept_id</th><th>gender</th><th>salary</th></tr></thead><tbody><tr><td>100</td><td>Stephen</td><td>1999</td><td>100</td><td>M</td><td>2000</td></tr><tr><td>200</td><td>Philipp</td><td>2002</td><td>200</td><td>M</td><td>8000</td></tr><tr><td>300</td><td>John</td><td>2010</td><td>100</td><td></td><td>6000</td></tr><tr><td>300</td><td>John</td><td>2010</td><td>100</td><td></td><td>6000</td></tr><tr><td>400</td><td>Nancy</td><td>2008</td><td>400</td><td>F</td><td>1000</td></tr><tr><td>500</td><td>Rosy</td><td>2014</td><td>500</td><td>M</td><td>5000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         100,
         "Stephen",
         "1999",
         "100",
         "M",
         2000
        ],
        [
         200,
         "Philipp",
         "2002",
         "200",
         "M",
         8000
        ],
        [
         300,
         "John",
         "2010",
         "100",
         "",
         6000
        ],
        [
         300,
         "John",
         "2010",
         "100",
         "",
         6000
        ],
        [
         400,
         "Nancy",
         "2008",
         "400",
         "F",
         1000
        ],
        [
         500,
         "Rosy",
         "2014",
         "500",
         "M",
         5000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doj",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_dept_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dF_unionAll = dF1.unionAll(dF2)\n",
    "\n",
    "display(dF_unionAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cf89f16-382a-4d33-bc6f-539e00ff5708",
     "showTitle": true,
     "title": "Schema Mismatch Case"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>name</th><th>doj</th><th>employee_dept_id</th><th>gender</th></tr></thead><tbody><tr><td>300</td><td>John</td><td>2010</td><td>100</td><td></td></tr><tr><td>400</td><td>Nancy</td><td>2008</td><td>400</td><td>F</td></tr><tr><td>500</td><td>Rosy</td><td>2014</td><td>500</td><td>M</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         300,
         "John",
         "2010",
         "100",
         ""
        ],
        [
         400,
         "Nancy",
         "2008",
         "400",
         "F"
        ],
        [
         500,
         "Rosy",
         "2014",
         "500",
         "M"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doj",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_dept_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dF3 = dF2.select(dF2.employee_id,dF2.name,dF2.doj,dF2.employee_dept_id,dF2.gender)\n",
    "\n",
    "display(dF3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1becf98-c1d6-4858-b589-316d45ffbffd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-26867747854029>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m dF_invalidScenerio \u001B[38;5;241m=\u001B[39m dF1\u001B[38;5;241m.\u001B[39munion(dF3)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     50\u001B[0m     )\n",
       "\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:4856\u001B[0m, in \u001B[0;36mDataFrame.union\u001B[0;34m(self, other)\u001B[0m\n",
       "\u001B[1;32m   4760\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21munion\u001B[39m(\u001B[38;5;28mself\u001B[39m, other: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[1;32m   4761\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Return a new :class:`DataFrame` containing the union of rows in this and another\u001B[39;00m\n",
       "\u001B[1;32m   4762\u001B[0m \u001B[38;5;124;03m    :class:`DataFrame`.\u001B[39;00m\n",
       "\u001B[1;32m   4763\u001B[0m \n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   4854\u001B[0m \u001B[38;5;124;03m    +---+-----+\u001B[39;00m\n",
       "\u001B[1;32m   4855\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 4856\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jdf\u001B[38;5;241m.\u001B[39munion(other\u001B[38;5;241m.\u001B[39m_jdf), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n",
       "\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n",
       "\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:254\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    250\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    251\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    252\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    253\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 254\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    255\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    256\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [NUM_COLUMNS_MISMATCH] UNION can only be performed on inputs with the same number of columns, but the first input has 6 columns and the second input has 5 columns. SQLSTATE: 42826;\n",
       "'Union false, false\n",
       ":- LogicalRDD [employee_id#2L, name#3, doj#4, employee_dept_id#5, gender#6, salary#7L], false\n",
       "+- Project [employee_id#27L, name#28, doj#29, employee_dept_id#30, gender#31]\n",
       "   +- LogicalRDD [employee_id#27L, name#28, doj#29, employee_dept_id#30, gender#31, salary#32L], false\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[NUM_COLUMNS_MISMATCH] UNION can only be performed on inputs with the same number of columns, but the first input has 6 columns and the second input has 5 columns. SQLSTATE: 42826;\n'Union false, false\n:- LogicalRDD [employee_id#2L, name#3, doj#4, employee_dept_id#5, gender#6, salary#7L], false\n+- Project [employee_id#27L, name#28, doj#29, employee_dept_id#30, gender#31]\n   +- LogicalRDD [employee_id#27L, name#28, doj#29, employee_dept_id#30, gender#31, salary#32L], false\n"
       },
       "metadata": {
        "errorSummary": "[NUM_COLUMNS_MISMATCH] UNION can only be performed on inputs with the same number of columns, but the first input has 6 columns and the second input has 5 columns. SQLSTATE: 42826"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "NUM_COLUMNS_MISMATCH",
        "sqlState": "42826",
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-26867747854029>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m dF_invalidScenerio \u001B[38;5;241m=\u001B[39m dF1\u001B[38;5;241m.\u001B[39munion(dF3)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     50\u001B[0m     )\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:4856\u001B[0m, in \u001B[0;36mDataFrame.union\u001B[0;34m(self, other)\u001B[0m\n\u001B[1;32m   4760\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21munion\u001B[39m(\u001B[38;5;28mself\u001B[39m, other: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   4761\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Return a new :class:`DataFrame` containing the union of rows in this and another\u001B[39;00m\n\u001B[1;32m   4762\u001B[0m \u001B[38;5;124;03m    :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m   4763\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4854\u001B[0m \u001B[38;5;124;03m    +---+-----+\u001B[39;00m\n\u001B[1;32m   4855\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 4856\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jdf\u001B[38;5;241m.\u001B[39munion(other\u001B[38;5;241m.\u001B[39m_jdf), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:254\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    250\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    251\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    252\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    253\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 254\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    255\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    256\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [NUM_COLUMNS_MISMATCH] UNION can only be performed on inputs with the same number of columns, but the first input has 6 columns and the second input has 5 columns. SQLSTATE: 42826;\n'Union false, false\n:- LogicalRDD [employee_id#2L, name#3, doj#4, employee_dept_id#5, gender#6, salary#7L], false\n+- Project [employee_id#27L, name#28, doj#29, employee_dept_id#30, gender#31]\n   +- LogicalRDD [employee_id#27L, name#28, doj#29, employee_dept_id#30, gender#31, salary#32L], false\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dF_invalidScenerio = dF1.union(dF3)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "08. Union and UnionAll",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
